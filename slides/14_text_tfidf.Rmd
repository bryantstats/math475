
---
title: "Analyzing frequency: tf-idf"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      
   

---
class: inverse, middle, center

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE)
```

---
Suppose we have a text data that contains $n$ document as below. 

|   |            |
|---|------------|
|   | Document 1 |
|   | Document 2 |
|   | ...        |
|   | Document n |

- A document could be a sentence, a paragraph 

- A document contains of `terms`.  Terms could be a word or a collection of words

- How important is a word/term to a document?

---
# Term frequency (tf)

- The more often the words show up the more important it is

- We could use `Term Frequency`   

- TF (Term Frequency) = (Number of times term `t` appears in a document) / (Total number of terms in the document).

- Please notice that the same word/term may have different `tf` in a different document

- For example: the word `love` in document 1 may have different `tf` when compared with that of document 2. 

---
# Term frequency (tf)

```{r}
library(tidyverse)
library(tidytext)
library(knitr)
library(kableExtra)
df <- read_csv('netflix_titles.csv')
titles = c('Spider-Man 3', 'Marvel Anime: Wolverine', 'House of Cards', 'Batman: The Killing Joke', 'Superman Returns')

df = df %>% 
  select(title, description) %>% 
  filter(title %in% titles)


df_words = df %>% 
  unnest_tokens(input = description, output = word) %>% 
  count(title, word, sort = TRUE)



total_words <- df_words %>% 
  group_by(title) %>% 
  summarize(total = sum(n))

df_words <- left_join(df_words, total_words)

df_tf = df_words %>% 
  group_by(title) %>% 
  mutate(tf = n/total)

df_tf
```

---



---
# Issues with Term frequency (tf)

- Sometimes: Rare terms are more informative than frequent terms

    - Example: “the”, “is”, “of”...

- We should remove some words such as “the”, “is”, “of” if we use `tf` as a measure of importance

- Or we could create a `weight` for a term so that the rare words would have higher weights.

- Inverse Document Frequency (idf) is such a weight


---
# Inverse Document Frequency (idf)

- IDF (Inverse Document Frequency) = log_e(Total number of texts / Number of texts with term `t` in it).

$$idf(t) = \ln \bigg(\frac{\text{Total number of texts}}{\text{Number of texts with term t in it}} \bigg)$$

- The idf of a rare term is high, whereas the idf of a frequent term is likely to be low

- The `idf` of a term is a constant throughout the document.  For example the word `love` in Document 1 should have the same `idf` as the word `love` in Document 2. 

---
# tf-idf


$$\text{tf-idf }(t) = \text{tf }(t) \cdot \text{idf }(t) $$

- The `tf-idf` of the same term may have different values in different documents.  For example the word `love` in Document 1 may have a different value `tf-idf` compared to the word `love` in Document 2. 

---
# tf-idf

```{r}
df_tf_idf <- df_words %>%
  bind_tf_idf(word, title, n)

df_tf_idf
```

---

```{r}
library(forcats)

df_tf_idf %>%
  group_by(title) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = title)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~title, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```

---
# bigram

```{r}
library(tidyr)
df <- read_csv('../data/amazon_reviews.csv')

df = df %>% 
  select(name, reviews.text) %>% 
  filter(name=='Fire Tablet, 7 Display, Wi-Fi, 8 GB - Includes Special Offers, Magenta') %>% 
  rename(texts = reviews.text)

df_bigrams <- df %>%
  unnest_tokens(input = texts, output = bigram, token = "ngrams", n = 2) %>% 
  filter(!is.na(bigram))


df_bigrams %>%
  count(bigram, sort = TRUE)

bigrams_separated <- df_bigrams %>%
    separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
    filter(!word1 %in% stop_words$word) %>%
    filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
    count(word1, word2, sort = TRUE)

library(igraph)
bigram_graph <- bigram_counts %>%
  filter(n > 50) %>%
  graph_from_data_frame()



library(ggraph)
set.seed(2024)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```




---
# trigram


```{r}

trigram_counts = df %>%
  unnest_tokens(input = texts, output = trigram, token = "ngrams", n = 3) %>%
  filter(!is.na(trigram)) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  count(word1, word2, word3, sort = TRUE)


trigram_graph <- trigram_counts %>%
  filter(n > 10) %>%
  graph_from_data_frame()

set.seed(2020)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(trigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

```










---

```{r}
df <- read_csv('../data/Amazon_Unlocked_Mobile.csv')

df = df %>% 
  select(`Product Name`, Reviews) %>% 
  rename(
    id = `Product Name`,
    texts = Reviews)

df %>% 
  unnest_tokens(input = texts, output = word) %>% 
  count(id, word, sort = TRUE)
```

